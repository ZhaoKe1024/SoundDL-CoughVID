{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56a93bcd-d8d8-46b7-96c0-51a056e6a952",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r'C:/Program Files (zk)/PythonFiles/AClassification/SoundDL-CoughVID')\n",
    "import os\n",
    "import yaml\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchaudio\n",
    "from pretrained.wav2vec import Wav2Vec\n",
    "from models.conv_vae import ConvVAE, vae_loss\n",
    "from models.classifiers import LSTM_Classifier, LSTM_Attn_Classifier\n",
    "from modules.loss import FocalLoss\n",
    "from readers.coughvid_reader import CoughVID_Class, CoughVID_Dataset\n",
    "from readers.featurizer import Wave2Mel\n",
    "from readers.collate_fn import collate_fn\n",
    "from tools.plotter import calc_accuracy, plot_heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e00b104e-936a-4484-8560-187fe7b620ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始数据： (6341, 7)\n",
      "             filename\n",
      "status_full          \n",
      "0                2114\n",
      "1                3288\n",
      "2                 939\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "src_data = pd.read_csv(\"./datasets/waveinfo_labedfine_forcls.csv\", header=0, index_col=0, delimiter=',')\n",
    "print(\"原始数据：\", src_data.shape)\n",
    "print(src_data.iloc[:, [0, 6]].groupby(\"status_full\").count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc4b3d35-dbb0-4efe-b3ab-f596e1442c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Pretrained model Wav2Vec...\n",
      "Create CrossEntropyLoss...\n",
      "All model and loss are on device: cuda\n",
      "Create TDNN, Adam with lr=1e-3, CosineAnnealingLR Shceduler\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "encoder = Wav2Vec(pretrained=True).to(device)\n",
    "print(\"Load Pretrained model Wav2Vec...\")\n",
    "\n",
    "criterion = FocalLoss(class_num=3)\n",
    "print(\"Create CrossEntropyLoss...\")\n",
    "\n",
    "print(\"All model and loss are on device:\", device)\n",
    "\n",
    "model = LSTM_Classifier(inp_size=298, hidden_size=64, n_classes=3).to(device)\n",
    "\n",
    "# model loss_function optimizer scheduler\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=1e-4, max_lr=1e-1, step_size_up=10)\n",
    "print(\"Create TDNN, Adam with lr=1e-3, CosineAnnealingLR Shceduler\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63a1f64e-b147-4032-96d6-8a90f90c7776",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = {\n",
    "    \"run_save_dir\": \"./runs/wav2vec_coughvid/\",\n",
    "    \"model\":{\n",
    "        \"num_class\": 3,\n",
    "        \"input_length\": 94,\n",
    "        \"wav_length\": 48000,\n",
    "        \"input_dim\": 512,\n",
    "        \"n_mels\": 128,\n",
    "        },\n",
    "    \"fit\":{\n",
    "        \"batch_size\": 32,\n",
    "        \"epochs\" : 23,\n",
    "        \"start_scheduler_epoch\": 6\n",
    "        },\n",
    "}\n",
    "\n",
    "num_epoch = configs[\"fit\"][\"epochs\"]\n",
    "# klw = 0.00025\n",
    "# istrain: 如果是评估环节，设为False，读取测试集，并且不创建optimizer\n",
    "# isdemo: 如果只是测试一下，设为True，仅读取32条数据方便快速测试是否有bug\n",
    "# istrain, isdemo = True, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe776c8c-dc24-4d01-88ac-12ffb8f03c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of trainingset:  6044 6044\n",
      "num of testingset: 297 297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading:   0%|                                                                                | 0/6044 [00:00<?, ?it/s]C:\\Program Files (zk)\\PythonFiles\\AClassification\\SoundDL-CoughVID\\readers\\audio.py:120: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  samples, sample_rate = librosa.load(file)  # , dtype='float32')\n",
      "C:\\Users\\zhaoke\\miniconda3\\envs\\torch-0\\lib\\site-packages\\librosa\\core\\audio.py:183: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
      "Loading:   0%|                                                                      | 1/6044 [00:00<1:12:15,  1.39it/s]C:\\Program Files (zk)\\PythonFiles\\AClassification\\SoundDL-CoughVID\\readers\\audio.py:120: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  samples, sample_rate = librosa.load(file)  # , dtype='float32')\n",
      "C:\\Users\\zhaoke\\miniconda3\\envs\\torch-0\\lib\\site-packages\\librosa\\core\\audio.py:183: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
      "Loading:   0%|                                                                        | 5/6044 [00:00<14:38,  6.87it/s]C:\\Program Files (zk)\\PythonFiles\\AClassification\\SoundDL-CoughVID\\readers\\audio.py:120: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  samples, sample_rate = librosa.load(file)  # , dtype='float32')\n",
      "C:\\Users\\zhaoke\\miniconda3\\envs\\torch-0\\lib\\site-packages\\librosa\\core\\audio.py:183: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
      "Loading: 100%|█████████████████████████████████████████████████████████████████████| 6044/6044 [06:55<00:00, 14.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset Creat Completely, cost time: 415.5861785411835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading: 100%|███████████████████████████████████████████████████████████████████████| 297/297 [00:21<00:00, 13.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Dataset Creat Completely, cost time: 21.309322834014893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_x, train_y, test_x, test_y = CoughVID_Class(isdemo=False)\n",
    "\n",
    "tic = time.time()\n",
    "cough_dataset = CoughVID_Dataset(path_list=train_x, label_list=train_y)\n",
    "toc = time.time()\n",
    "print(\"Train Dataset Creat Completely, cost time:\", toc-tic)\n",
    "\n",
    "tic = time.time()\n",
    "valid_dataset = CoughVID_Dataset(path_list=test_x, label_list=test_y)\n",
    "toc = time.time()\n",
    "print(\"Valid Dataset Creat Completely, cost time:\", toc-tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "504a7f69-27eb-4991-803e-3393fe67aabe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create Training Loader and Valid Loader.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_loader = DataLoader(cough_dataset, batch_size=32, shuffle=True,\n",
    "                          collate_fn=collate_fn)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=True,\n",
    "                          collate_fn=collate_fn)\n",
    "# for i, (x_wav, y_label, max_len_rate) in enumerate(train_loader):\n",
    "#     print(x_wav.shape)\n",
    "#     print(y_label)\n",
    "#     print(max_len_rate)\n",
    "#     x_mel = w2m(x_wav)\n",
    "#     print(x_mel[0])\n",
    "#     break\n",
    "print(\"Create Training Loader and Valid Loader.\")\n",
    "\n",
    "# w2m = Wave2Mel(sr=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b0d2832f-bdf9-42be-bd2f-3543aa4acb64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 0, 1, 0, 2, 0, 1, 2, 1, 1, 1, 1, 2, 1, 0, 1, 0, 0, 1, 1, 2, 2,\n",
      "        2, 0, 0, 2, 1, 2, 2, 1])\n",
      "tensor([[0.0305, 0.0000, 0.0000,  ..., 0.0246, 0.0000, 0.0000],\n",
      "        [0.0454, 0.0000, 0.0000,  ..., 0.0314, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0833, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0588, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0059, 0.0000,  ..., 0.0290, 0.0000, 0.0000]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "tensor([1, 0, 0, 1, 1, 0, 2, 2, 2, 0, 1, 2, 2, 1, 2, 0, 0, 0, 2, 0, 2, 2, 2, 0,\n",
      "        1, 2, 0, 2, 1, 0, 1, 0])\n",
      "tensor([[0.0305, 0.0000, 0.0000,  ..., 0.0246, 0.0000, 0.0000],\n",
      "        [0.0454, 0.0000, 0.0000,  ..., 0.0314, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0833, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0588, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0059, 0.0000,  ..., 0.0290, 0.0000, 0.0000]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "tensor([1, 1, 0, 2, 1, 2, 1, 0, 2, 2, 0, 2, 2, 2, 1, 2, 1, 2, 0, 2, 0, 1, 1, 2,\n",
      "        1, 2, 0, 0, 2, 2, 0, 0])\n",
      "tensor([[0.0305, 0.0000, 0.0000,  ..., 0.0246, 0.0000, 0.0000],\n",
      "        [0.0454, 0.0000, 0.0000,  ..., 0.0314, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0833, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0588, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0059, 0.0000,  ..., 0.0290, 0.0000, 0.0000]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "tensor([1, 2, 2, 2, 0, 0, 0, 2, 1, 0, 0, 0, 2, 2, 1, 0, 1, 1, 1, 2, 0, 2, 2, 0,\n",
      "        2, 0, 1, 2, 0, 0, 2, 2])\n",
      "tensor([[0.0305, 0.0000, 0.0000,  ..., 0.0246, 0.0000, 0.0000],\n",
      "        [0.0454, 0.0000, 0.0000,  ..., 0.0314, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0833, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0588, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0059, 0.0000,  ..., 0.0290, 0.0000, 0.0000]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "tensor([2, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 2, 0, 1, 1, 2, 2, 0, 2, 1, 2,\n",
      "        2, 2, 0, 0, 2, 1, 2, 1])\n",
      "tensor([[0.0305, 0.0000, 0.0000,  ..., 0.0246, 0.0000, 0.0000],\n",
      "        [0.0454, 0.0000, 0.0000,  ..., 0.0314, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0833, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0588, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0059, 0.0000,  ..., 0.0290, 0.0000, 0.0000]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for i, (x_wav, y_label, max_len_rate) in enumerate(valid_loader):\n",
    "    # print(x_wav.shape)\n",
    "    print(y_label)\n",
    "    # print(max_len_rate)\n",
    "    # x_mel = w2m(x_wav)\n",
    "    print(x_mel[0])\n",
    "    if i>3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0984c8c7-d09f-452d-9193-b4dca2bebde1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "创建运行保存文件 ./runs/wav2vec_coughvid/202404301024_tdnn_focalloss/\n"
     ]
    }
   ],
   "source": [
    "timestr = time.strftime(\"%Y%m%d%H%M\", time.localtime())\n",
    "run_save_dir = configs[\"run_save_dir\"] + timestr + f'_tdnn_focalloss/'\n",
    "os.makedirs(run_save_dir, exist_ok=True)\n",
    "print(\"创建运行保存文件\", run_save_dir)\n",
    "with open(\"setting.txt\", 'w') as fout:\n",
    "    fout.write(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62edce9b-8c28-46c2-b1aa-47f63619da86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|                                                                                | 0/189 [00:00<?, ?it/s]C:\\Users\\zhaoke\\AppData\\Local\\Temp\\ipykernel_13644\\1972210870.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_label = torch.tensor(y_label, device=device)\n",
      "Training:   2%|█▌                                                                      | 4/189 [00:01<01:11,  2.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[0], mtid pred loss:0.4710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  33%|███████████████████████▎                                               | 62/189 [00:06<00:09, 13.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[0], mtid pred loss:0.4593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  65%|█████████████████████████████████████████████▏                        | 122/189 [00:10<00:04, 13.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[0], mtid pred loss:0.4791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  96%|███████████████████████████████████████████████████████████████████▍  | 182/189 [00:14<00:00, 13.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[0], mtid pred loss:0.4651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████████████████████████████| 189/189 [00:15<00:00, 12.24it/s]\n",
      "Validate:   0%|                                                                                 | 0/10 [00:00<?, ?it/s]C:\\Users\\zhaoke\\AppData\\Local\\Temp\\ipykernel_13644\\1972210870.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_label = torch.tensor(y_label, device=device)\n",
      "Validate:  30%|█████████████████████▉                                                   | 3/10 [00:00<00:00, 19.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 298, 512])\n",
      "torch.Size([32, 298, 512])\n",
      "torch.Size([32, 298, 512])\n",
      "torch.Size([32, 298, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validate:  50%|████████████████████████████████████▌                                    | 5/10 [00:00<00:00, 17.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 298, 512])\n",
      "torch.Size([32, 298, 512])\n",
      "torch.Size([32, 298, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validate: 100%|████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 17.34it/s]\n",
      "C:\\Program Files (zk)\\PythonFiles\\AClassification\\SoundDL-CoughVID\\tools\\plotter.py:132: RuntimeWarning: invalid value encountered in divide\n",
      "  prec = tp_vec / cfm.sum(axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 298, 512])\n",
      "torch.Size([32, 298, 512])\n",
      "torch.Size([9, 298, 512])\n",
      "heatmap_input shape: torch.Size([297, 3])\n",
      "lables shape: torch.Size([297])\n",
      "(297, 3)\n",
      "acc: 0.3333333333333333\n",
      "precision: ['nan', '0.3333', 'nan']\n",
      "recall: ['0.0000', '1.0000', '0.0000']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|                                                                                | 0/189 [00:00<?, ?it/s]C:\\Users\\zhaoke\\AppData\\Local\\Temp\\ipykernel_13644\\1972210870.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_label = torch.tensor(y_label, device=device)\n",
      "Training:   1%|▊                                                                       | 2/189 [00:00<00:10, 18.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1], mtid pred loss:0.4415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  33%|███████████████████████▎                                               | 62/189 [00:04<00:08, 14.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1], mtid pred loss:0.4510\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  65%|█████████████████████████████████████████████▏                        | 122/189 [00:08<00:04, 13.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1], mtid pred loss:0.4436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  96%|███████████████████████████████████████████████████████████████████▍  | 182/189 [00:13<00:00, 14.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1], mtid pred loss:0.4424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████████████████████████████| 189/189 [00:13<00:00, 13.80it/s]\n",
      "Validate:   0%|                                                                                 | 0/10 [00:00<?, ?it/s]C:\\Users\\zhaoke\\AppData\\Local\\Temp\\ipykernel_13644\\1972210870.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_label = torch.tensor(y_label, device=device)\n",
      "Validate:  40%|█████████████████████████████▏                                           | 4/10 [00:00<00:00, 32.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 298, 512])\n",
      "torch.Size([32, 298, 512])\n",
      "torch.Size([32, 298, 512])\n",
      "torch.Size([32, 298, 512])\n",
      "torch.Size([32, 298, 512])\n",
      "torch.Size([32, 298, 512])\n",
      "torch.Size([32, 298, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validate: 100%|████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 34.25it/s]\n",
      "C:\\Program Files (zk)\\PythonFiles\\AClassification\\SoundDL-CoughVID\\tools\\plotter.py:132: RuntimeWarning: invalid value encountered in divide\n",
      "  prec = tp_vec / cfm.sum(axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 298, 512])\n",
      "torch.Size([32, 298, 512])\n",
      "torch.Size([9, 298, 512])\n",
      "heatmap_input shape: torch.Size([297, 3])\n",
      "lables shape: torch.Size([297])\n",
      "(297, 3)\n",
      "acc: 0.3333333333333333\n",
      "precision: ['nan', '0.3333', 'nan']\n",
      "recall: ['0.0000', '1.0000', '0.0000']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|                                                                                | 0/189 [00:00<?, ?it/s]C:\\Users\\zhaoke\\AppData\\Local\\Temp\\ipykernel_13644\\1972210870.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_label = torch.tensor(y_label, device=device)\n",
      "Training:   1%|▊                                                                       | 2/189 [00:00<00:09, 19.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[2], mtid pred loss:0.4530\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  33%|███████████████████████▎                                               | 62/189 [00:04<00:09, 13.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[2], mtid pred loss:0.4525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  65%|█████████████████████████████████████████████▏                        | 122/189 [00:08<00:04, 13.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[2], mtid pred loss:0.4237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  97%|████████████████████████████████████████████████████████████████████▏ | 184/189 [00:13<00:00, 14.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[2], mtid pred loss:0.4216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████████████████████████████| 189/189 [00:13<00:00, 13.99it/s]\n",
      "Validate:   0%|                                                                                 | 0/10 [00:00<?, ?it/s]C:\\Users\\zhaoke\\AppData\\Local\\Temp\\ipykernel_13644\\1972210870.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_label = torch.tensor(y_label, device=device)\n",
      "Validate:  40%|█████████████████████████████▏                                           | 4/10 [00:00<00:00, 30.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 298, 512])\n",
      "torch.Size([32, 298, 512])\n",
      "torch.Size([32, 298, 512])\n",
      "torch.Size([32, 298, 512])\n",
      "torch.Size([32, 298, 512])\n",
      "torch.Size([32, 298, 512])\n",
      "torch.Size([32, 298, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validate: 100%|████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 33.79it/s]\n",
      "C:\\Program Files (zk)\\PythonFiles\\AClassification\\SoundDL-CoughVID\\tools\\plotter.py:132: RuntimeWarning: invalid value encountered in divide\n",
      "  prec = tp_vec / cfm.sum(axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 298, 512])\n",
      "torch.Size([32, 298, 512])\n",
      "torch.Size([9, 298, 512])\n",
      "heatmap_input shape: torch.Size([297, 3])\n",
      "lables shape: torch.Size([297])\n",
      "(297, 3)\n",
      "acc: 0.3333333333333333\n",
      "precision: ['nan', '0.3333', 'nan']\n",
      "recall: ['0.0000', '1.0000', '0.0000']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|                                                                                | 0/189 [00:00<?, ?it/s]C:\\Users\\zhaoke\\AppData\\Local\\Temp\\ipykernel_13644\\1972210870.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_label = torch.tensor(y_label, device=device)\n",
      "Training:   1%|▊                                                                       | 2/189 [00:00<00:10, 18.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[3], mtid pred loss:0.4525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  33%|███████████████████████▎                                               | 62/189 [00:04<00:09, 13.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[3], mtid pred loss:0.4431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  65%|█████████████████████████████████████████████▏                        | 122/189 [00:08<00:04, 13.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[3], mtid pred loss:0.4219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  96%|███████████████████████████████████████████████████████████████████▍  | 182/189 [00:13<00:00, 13.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[3], mtid pred loss:0.4467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████████████████████████████| 189/189 [00:13<00:00, 13.69it/s]\n",
      "Validate:   0%|                                                                                 | 0/10 [00:00<?, ?it/s]C:\\Users\\zhaoke\\AppData\\Local\\Temp\\ipykernel_13644\\1972210870.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_label = torch.tensor(y_label, device=device)\n",
      "Validate:  40%|█████████████████████████████▏                                           | 4/10 [00:00<00:00, 31.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 298, 512])\n",
      "torch.Size([32, 298, 512])\n",
      "torch.Size([32, 298, 512])\n",
      "torch.Size([32, 298, 512])\n",
      "torch.Size([32, 298, 512])\n",
      "torch.Size([32, 298, 512])\n",
      "torch.Size([32, 298, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validate: 100%|████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 33.24it/s]\n",
      "C:\\Program Files (zk)\\PythonFiles\\AClassification\\SoundDL-CoughVID\\tools\\plotter.py:132: RuntimeWarning: invalid value encountered in divide\n",
      "  prec = tp_vec / cfm.sum(axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 298, 512])\n",
      "torch.Size([32, 298, 512])\n",
      "torch.Size([9, 298, 512])\n",
      "heatmap_input shape: torch.Size([297, 3])\n",
      "lables shape: torch.Size([297])\n",
      "(297, 3)\n",
      "acc: 0.3333333333333333\n",
      "precision: ['nan', '0.3333', 'nan']\n",
      "recall: ['0.0000', '1.0000', '0.0000']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|                                                                                | 0/189 [00:00<?, ?it/s]C:\\Users\\zhaoke\\AppData\\Local\\Temp\\ipykernel_13644\\1972210870.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_label = torch.tensor(y_label, device=device)\n",
      "Training:   1%|▊                                                                       | 2/189 [00:00<00:10, 17.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[4], mtid pred loss:0.4392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  30%|█████████████████████▍                                                 | 57/189 [00:04<00:09, 13.29it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x_idx \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m---> 19\u001b[0m     history1\u001b[38;5;241m.\u001b[39mappend(\u001b[43mpred_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x_idx \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m60\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m], mtid pred loss:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpred_loss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history1 = []\n",
    "for epoch_id in range(configs[\"fit\"][\"epochs\"]):\n",
    "    # ---------------------------\n",
    "    # -----------TRAIN-----------\n",
    "    # ---------------------------\n",
    "    model.train()\n",
    "    for x_idx, (x_wav, y_label, _) in enumerate(tqdm(train_loader, desc=\"Training\")):\n",
    "        x_wav = x_wav.to(device)\n",
    "        x_mel = encoder(x_wav).transpose(1,2)\n",
    "        y_label = torch.tensor(y_label, device=device)\n",
    "        # print(\"shape of x_mel:\", x_mel.shape)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        y_hat = model(x_mel)\n",
    "        pred_loss = criterion(y_hat, y_label)\n",
    "        pred_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if x_idx > 2:\n",
    "            history1.append(pred_loss.item())\n",
    "        if x_idx % 60 == 0:\n",
    "            print(f\"Epoch[{epoch_id}], mtid pred loss:{pred_loss.item():.4f}\")\n",
    "    if epoch_id >= configs[\"fit\"][\"start_scheduler_epoch\"]:\n",
    "        scheduler.step()\n",
    "\n",
    "    # ---------------------------\n",
    "    # -----------SAVE------------\n",
    "    # ---------------------------\n",
    "    plt.figure(0)\n",
    "    plt.plot(range(len(history1)), history1, c=\"green\", alpha=0.7)\n",
    "    plt.savefig(run_save_dir + f'cls_loss_iter_{epoch_id}.png')\n",
    "    plt.close()\n",
    "    # if epoch > 6 and epoch % 2 == 0:\n",
    "    os.makedirs(run_save_dir + f\"model_epoch_{epoch_id}/\", exist_ok=True)\n",
    "    tmp_model_path = \"{model}model_{epoch}.pth\".format(\n",
    "        model=run_save_dir + f\"model_epoch_{epoch_id}/\",\n",
    "        epoch=epoch_id)\n",
    "    torch.save(model.state_dict(), tmp_model_path)\n",
    "    # ---------------------------\n",
    "    # -----------TEST------------\n",
    "    # ---------------------------\n",
    "    model.eval()\n",
    "    heatmap_input = None\n",
    "    labels = None\n",
    "    for x_idx, (x_wav, y_label, _) in enumerate(tqdm(valid_loader, desc=\"Validate\")):\n",
    "        x_wav = x_wav.to(device)\n",
    "        x_mel = encoder(x_wav).transpose(1,2)\n",
    "        print(x_mel.shape)\n",
    "        y_label = torch.tensor(y_label, device=device)\n",
    "        \n",
    "        y_pred = model(x_mel)\n",
    "        pred_loss = criterion(y_pred, y_label)\n",
    "        \n",
    "        if x_idx == 0:\n",
    "            heatmap_input, labels = y_pred, y_label\n",
    "        else:\n",
    "            heatmap_input = torch.concat((heatmap_input, y_pred), dim=0)\n",
    "            labels = torch.concat((labels, y_label), dim=0)\n",
    "        # if x_idx * configs[\"fit\"][\"batch_size\"] > 800:\n",
    "        #     break\n",
    "    print(\"heatmap_input shape:\", heatmap_input.shape)\n",
    "    print(\"lables shape:\", labels.shape)\n",
    "    # if epoch > 3:\n",
    "    #     self.plot_reduction(resume_path=\"\", load_epoch=epoch, reducers=[\"heatmap\"])\n",
    "    heatmap_input = heatmap_input.detach().cpu().numpy()\n",
    "    labels = labels.detach().cpu().numpy()\n",
    "    calc_accuracy(pred_matrix=heatmap_input, label_vec=labels,\n",
    "                  save_path=run_save_dir + f\"/accuracy_epoch_{epoch_id}.txt\")\n",
    "    plot_heatmap(pred_matrix=heatmap_input, label_vec=labels,\n",
    "                 ticks=[\"healthy\", \"symptomatic\", \"COVID-19\"],\n",
    "                 save_path=run_save_dir + f\"/heatmap_epoch_{epoch_id}.png\")\n",
    "print(\"============== END TRAINING ==============\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

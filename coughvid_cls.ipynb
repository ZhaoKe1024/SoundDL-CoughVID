{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56a93bcd-d8d8-46b7-96c0-51a056e6a952",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r'C:/Program Files (zk)/PythonFiles/AClassification/SoundDL-CoughVID')\n",
    "import os\n",
    "import yaml\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchaudio\n",
    "from models.conv_vae import ConvVAE, vae_loss\n",
    "from readers.coughvid_reader import CoughVID_Class, CoughVID_Dataset\n",
    "from readers.featurizer import Wave2Mel\n",
    "from pretrained.wav2vec import Wav2Vec\n",
    "from readers.collate_fn import collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bda03ce8-db9a-4251-bae2-8568dac3522b",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Wav2Vec(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e00b104e-936a-4484-8560-187fe7b620ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始数据： (6341, 7)\n",
      "             filename\n",
      "status_full          \n",
      "0                2114\n",
      "1                3288\n",
      "2                 939\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "src_data = pd.read_csv(\"./datasets/waveinfo_labedfine_forcls.csv\", header=0, index_col=0, delimiter=',')\n",
    "print(\"原始数据：\", src_data.shape)\n",
    "print(src_data.iloc[:, [0, 6]].groupby(\"status_full\").count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe776c8c-dc24-4d01-88ac-12ffb8f03c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_x, train_y, test_x, test_y = CoughVID_Class(isdemo=True)\n",
    "\n",
    "tic = time.time()\n",
    "cough_dataset = CoughVID_Dataset(train_x, train_y)\n",
    "toc = time.time()\n",
    "print(\"Train Dataset Creat Completely, cost time:\", toc-tic)\n",
    "\n",
    "tic = time.time()\n",
    "valid_dataset = CoughVID_Dataset(path_list=test_x, label_list=test_y)\n",
    "toc = time.time()\n",
    "print(\"Valid Dataset Creat Completely, cost time:\", toc-tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63a1f64e-b147-4032-96d6-8a90f90c7776",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = {\n",
    "    \"run_save_dir\": \"./runs/wav2vec_coughvid/\",\n",
    "    \"model\":{\n",
    "        \"num_class\": 3,\n",
    "        \"input_length\": 94,\n",
    "        \"wav_length\": 48000,\n",
    "        \"input_dim\": 512,\n",
    "        \"n_mels\": 128,\n",
    "        },\n",
    "    \"fit\":{\n",
    "        \"batch_size\": 64,\n",
    "        \"epochs\" : 23,\n",
    "        \"start_scheduler_epoch\": 6\n",
    "        },\n",
    "}\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else \"cpu\"\n",
    "num_epoch = configs[\"fit\"][\"epochs\"]\n",
    "# klw = 0.00025\n",
    "# istrain: 如果是评估环节，设为False，读取测试集，并且不创建optimizer\n",
    "# isdemo: 如果只是测试一下，设为True，仅读取32条数据方便快速测试是否有bug\n",
    "# istrain, isdemo = True, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "350227f6-96cc-43a1-8b7b-8645b39fafb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Classifier(nn.Module):\n",
    "    def __init__(self, inp_size, hidden_size, n_classes):\n",
    "        super(LSTM_Classifier, self).__init__()\n",
    "        self.lstm = nn.LSTM(inp_size, hidden_size, batch_first=True)\n",
    "        self.classifier = nn.Linear(hidden_size, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, (hidden, _) = self.lstm(x.transpose(1,2))\n",
    "        lstm_out = lstm_out[:, -1, :]\n",
    "        out = self.classifier(lstm_out)\n",
    "        return out\n",
    "        \n",
    "class LSTM_Attn_Classifier(nn.Module):\n",
    "    def __init__(self, inp_size, hidden_size, n_classes, return_attn_weights=False, attn_type='dot'):\n",
    "        super(LSTM_Attn_Classifier, self).__init__()\n",
    "        self.return_attn_weights = return_attn_weights\n",
    "        self.lstm = nn.LSTM(inp_size, hidden_size, batch_first=True)\n",
    "        self.attn_type = attn_type\n",
    "\n",
    "        if self.attn_type == 'dot':\n",
    "            self.attention = DotAttention()\n",
    "        elif self.attn_type == 'soft':\n",
    "            self.attention = SoftAttention(hidden_size, hidden_size)\n",
    "\n",
    "        self.classifier = nn.Linear(hidden_size, n_classes)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, (hidden, _) = self.lstm(x.transpose(1,2))\n",
    "\n",
    "        if self.attn_type == 'dot':\n",
    "            attn_output = self.attention(lstm_out, hidden)\n",
    "            attn_weights = self.attention._get_weights(lstm_out, hidden)\n",
    "        elif self.attn_type == 'soft':\n",
    "            attn_output = self.attention(lstm_out)\n",
    "            attn_weights = self.attention._get_weights(lstm_out)\n",
    "\n",
    "        out = self.classifier(attn_output)\n",
    "        if self.return_attn_weights:\n",
    "            return out, attn_weights\n",
    "        else:\n",
    "            return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bc4b3d35-dbb0-4efe-b3ab-f596e1442c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All model and loss are on device: cuda\n",
      "Create TDNN, Adam with lr=1e-3, CosineAnnealingLR Shceduler\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# model loss_function optimizer scheduler\n",
    "print(\"All model and loss are on device:\", device)\n",
    "model = LSTM_Classifier(inp_size=298, hidden_size=64, n_classes=3).to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=1e-4, max_lr=1e-1, step_size_up=10)\n",
    "print(\"Create TDNN, Adam with lr=1e-3, CosineAnnealingLR Shceduler\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "504a7f69-27eb-4991-803e-3393fe67aabe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All model and loss are on device: cuda\n",
      "Create TDNN, Adam with lr=1e-3, CosineAnnealingLR Shceduler\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m criterion\u001b[38;5;241m=\u001b[39mnn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# print(\"Create VAELoss...\")\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(\u001b[43mtrain_dataset\u001b[49m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     15\u001b[0m                               collate_fn\u001b[38;5;241m=\u001b[39mcollate_fn)\n\u001b[0;32m     16\u001b[0m valid_loader \u001b[38;5;241m=\u001b[39m DataLoader(valid_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     17\u001b[0m                               collate_fn\u001b[38;5;241m=\u001b[39mcollate_fn)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreate Training Loader and Valid Loader.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "criterion=nn.CrossEntropyLoss().to(device)\n",
    "# print(\"Create VAELoss...\")\n",
    "\n",
    "train_loader = DataLoader(cough_dataset, batch_size=32, shuffle=False,\n",
    "                          collate_fn=collate_fn)\n",
    "test_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False,\n",
    "                          collate_fn=collate_fn)\n",
    "for i, (x_wav, y_label, max_len_rate) in enumerate(train_loader):\n",
    "    print(x_wav.shape)\n",
    "    print(y_label)\n",
    "    print(max_len_rate)\n",
    "    x_mel = w2m(x_wav)\n",
    "    print(x_mel[0])\n",
    "    break\n",
    "print(\"Create Training Loader and Valid Loader.\")\n",
    "\n",
    "# w2m = Wave2Mel(sr=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0984c8c7-d09f-452d-9193-b4dca2bebde1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "创建运行保存文件 ./runs/wav2vec_coughvid/202404291803_tdnn_focalloss/\n"
     ]
    }
   ],
   "source": [
    "timestr = time.strftime(\"%Y%m%d%H%M\", time.localtime())\n",
    "run_save_dir = configs[\"run_save_dir\"] + timestr + f'_tdnn_focalloss/'\n",
    "os.makedirs(run_save_dir, exist_ok=True)\n",
    "print(\"创建运行保存文件\", run_save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "62edce9b-8c28-46c2-b1aa-47f63619da86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|                                                                                | 0/189 [00:00<?, ?it/s]C:\\Users\\zhaoke\\AppData\\Local\\Temp\\ipykernel_2752\\1291739228.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_label = torch.tensor(y_label, device=device)\n",
      "Training:   1%|▍                                                                       | 1/189 [00:06<19:20,  6.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[0], mtid pred loss:1.0374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  32%|██████████████████████▉                                                | 61/189 [05:57<12:21,  5.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[0], mtid pred loss:0.9923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  64%|████████████████████████████████████████████▊                         | 121/189 [11:48<06:39,  5.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[0], mtid pred loss:1.1001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  96%|███████████████████████████████████████████████████████████████████   | 181/189 [17:38<00:46,  5.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[0], mtid pred loss:1.2097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████████████████████████████| 189/189 [18:24<00:00,  5.84s/it]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'valid_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 43\u001b[0m\n\u001b[0;32m     41\u001b[0m heatmap_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     42\u001b[0m labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x_idx, (x_wav, y_label, _) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm(\u001b[43mvalid_loader\u001b[49m, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidate\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[0;32m     44\u001b[0m     x_mel \u001b[38;5;241m=\u001b[39m encoder(x_wav)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     45\u001b[0m     y_label \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(y_label, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'valid_loader' is not defined"
     ]
    }
   ],
   "source": [
    "history1 = []\n",
    "for epoch_id in range(configs[\"fit\"][\"epochs\"]):\n",
    "    # ---------------------------\n",
    "    # -----------TRAIN-----------\n",
    "    # ---------------------------\n",
    "    model.train()\n",
    "    for x_idx, (x_wav, y_label, _) in enumerate(tqdm(train_loader, desc=\"Training\")):\n",
    "        x_mel = encoder(x_wav).transpose(1,2).to(device)\n",
    "        y_label = torch.tensor(y_label, device=device)\n",
    "        # print(\"shape of x_mel:\", x_mel.shape)\n",
    "        optimizer.zero_grad()\n",
    "        y_hat = model(x_mel)\n",
    "        pred_loss = criterion(y_hat, y_label)\n",
    "        pred_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if x_idx > 2:\n",
    "            history1.append(pred_loss.item())\n",
    "        if x_idx % 60 == 0:\n",
    "            print(f\"Epoch[{epoch_id}], mtid pred loss:{pred_loss.item():.4f}\")\n",
    "    if epoch_id >= configs[\"fit\"][\"start_scheduler_epoch\"]:\n",
    "        scheduler.step()\n",
    "\n",
    "    # ---------------------------\n",
    "    # -----------SAVE------------\n",
    "    # ---------------------------\n",
    "    plt.figure(0)\n",
    "    plt.plot(range(len(history1)), history1, c=\"green\", alpha=0.7)\n",
    "    plt.savefig(run_save_dir + f'cls_loss_iter_{epoch_id}.png')\n",
    "    plt.close()\n",
    "    # if epoch > 6 and epoch % 2 == 0:\n",
    "    os.makedirs(run_save_dir + f\"model_epoch_{epoch_id}/\", exist_ok=True)\n",
    "    tmp_model_path = \"{model}model_{epoch}.pth\".format(\n",
    "        model=run_save_dir + f\"model_epoch_{epoch_id}/\",\n",
    "        epoch=epoch_id)\n",
    "    torch.save(model.state_dict(), tmp_model_path)\n",
    "    # ---------------------------\n",
    "    # -----------TEST------------\n",
    "    # ---------------------------\n",
    "    model.eval()\n",
    "    heatmap_input = None\n",
    "    labels = None\n",
    "    for x_idx, (x_wav, y_label, _) in enumerate(tqdm(valid_loader, desc=\"Validate\")):\n",
    "        x_mel = encoder(x_wav).transpose(1,2).to(device)\n",
    "        y_label = torch.tensor(y_label, device=device)\n",
    "        y_hat = model(x_mel)\n",
    "        pred_loss = criterion(y_hat, y_label)\n",
    "        if x_idx == 0:\n",
    "            heatmap_input, labels = y_pred, y_label\n",
    "        else:\n",
    "            heatmap_input = torch.concat((heatmap_input, y_pred), dim=0)\n",
    "            labels = torch.concat((labels, y_label), dim=0)\n",
    "        # if x_idx * configs[\"fit\"][\"batch_size\"] > 800:\n",
    "        #     break\n",
    "    print(\"heatmap_input shape:\", heatmap_input.shape)\n",
    "    print(\"lables shape:\", labels.shape)\n",
    "    # if epoch > 3:\n",
    "    #     self.plot_reduction(resume_path=\"\", load_epoch=epoch, reducers=[\"heatmap\"])\n",
    "    heatmap_input = heatmap_input.detach().cpu().numpy()\n",
    "    labels = labels.detach().cpu().numpy()\n",
    "    calc_accuracy(pred_matrix=heatmap_input, label_vec=labels,\n",
    "                  save_path=run_save_dir + f\"/accuracy_epoch_{epoch_id}.txt\")\n",
    "    plot_heatmap(pred_matrix=heatmap_input, label_vec=labels,\n",
    "                 ticks=[\"healthy\", \"symptomatic\", \"COVID-19\"],\n",
    "                 save_path=run_save_dir + f\"/heatmap_epoch_{epoch_id}.png\")\n",
    "print(\"============== END TRAINING ==============\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
